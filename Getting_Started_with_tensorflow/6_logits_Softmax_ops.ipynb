{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of the function worth mentioning ( in building machine learning model ) is the softmax function. Its corresponding implementation in tensorflow is tf.nn.softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><b> tf.nn.softmax( ) </b>:</p>\n",
    "<p>This produces  the result of applying the softmax function to an input tensor.</p>\n",
    "\n",
    " $$\\sigma(z)_j = \\frac{e^{z_j}} {\\Sigma_{k=1}^K e^{z_k} }  $$  \n",
    " $$ for \\quad  j  =  1, â€¦, K $$\n",
    "\n",
    "\n",
    "\n",
    "</p>\n",
    "\n",
    "<p> The softmax \"squishes\" the inputs so that sum ( input ) = 1; it's a way of normalizing. The shape of output of a softmax is the same as the input - it just normalizes the values. The outputs of softmax can be interpreted as probabilities. </p>\n",
    "\n",
    "Source: http://stackoverflow.com/questions/34240703/difference-between-tensorflow-tf-nn-softmax-and-tf-nn-softmax-cross-entropy-with\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.16838508  0.205666    0.25120102  0.37474789]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:    \n",
    "    a = tf.constant(np.array([[.1, .3, .5, .9]]))\n",
    "    print sess.run(tf.nn.softmax(a))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.nn.softmax_cross_entropy_with_logits\n",
    "Computes softmax cross entropy between logits and labels. Use this in a classification problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>It computes the cross entropy of the result after applying the softmax function (but it does it all together in a more mathematically careful way). It's similar to the result of:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sm = tf.nn.softmax(x)\n",
    "ce = cross_entropy(sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use this in classification problem \n",
    "# y is prediction\n",
    "# label_nodes is target. \n",
    "# This becomes the cost function\n",
    "---cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, label_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The cross entropy is a summary metric - it sums across the elements. The output of tf.nn.softmax_cross_entropy_with_logits on a shape [2,5] tensor is of shape [2,1] (the first dimension is treated as the batch).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.reduce_mean( )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The arithmetic mean is the sum of the elements along the axis divided by the number of elements. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using Numpy without specifying axis\n",
    "array = np.array([[1, 2], [3, 4]])\n",
    "np.mean(array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  3.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#specify the axis\n",
    "np.mean(array, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.   5.]\n",
      " [  5.   0.]\n",
      " [  9.  12.]]\n",
      "('the result from numpy mean operation: ', array([  4. ,   2.5,  10.5]))\n",
      "('result from tf.mean: ', array([ 3.5,  5.5,  6.5]))\n"
     ]
    }
   ],
   "source": [
    "# The results is same as for Tensor flow\n",
    "array_mean = np.array([[3.,5], [5,0], [9.,12]])\n",
    "print(array_mean)\n",
    "print(\"the result from numpy mean operation: \", np.mean(array_mean,1))\n",
    "\n",
    "Mean = tf.reduce_mean(c,1)\n",
    "with tf.Session() as sess:\n",
    "    result = sess.run(Mean)\n",
    "    print(\"result from tf.mean: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tf.truncated_normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outputs random values from a truncated normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_constant and r_var are same values:\n",
      "[ 0.1  0.1  0.1  0.1]\n",
      "[ 0.1  0.1  0.1  0.1]\n",
      "\n",
      "\n",
      "weight initialised with truncated normal: \n",
      "[[ 0.95871228  0.28760111 -0.08963373  1.41321909  1.88666642]\n",
      " [-0.45240343 -0.42291233 -0.90050316  0.78051525 -0.66701406]\n",
      " [-1.77560961 -0.91545683  0.74849337 -1.31004107 -0.25731021]\n",
      " [-1.00727427 -0.69677335 -1.1047287   1.66000664  0.12934102]]\n"
     ]
    }
   ],
   "source": [
    "# We show how to initialise variable by a constant\n",
    "# And initialising using tf.constant\n",
    "\n",
    "y_constant = tf.constant(0.1, shape=[4]); #initialise a constant\n",
    "r_var = tf.Variable(y)   # initialise constant by variabel.\n",
    "# Both ways above can be used to initialise the bias variables                       \n",
    "weight = tf.Variable(tf.truncated_normal([4, 5])) # weight initialisation with tf.truncated_normal\n",
    "\n",
    "init_op = tf.global_variables_initializer() # initialise varianle\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    print(\"y_constant and r_var are same values:\")\n",
    "    print(y_constant.eval());\n",
    "\n",
    "    print(r_var.eval());print(\"\\n\");\n",
    "    print(\"weight initialised with truncated normal: \")\n",
    "\n",
    "    print(weight.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python2]",
   "language": "python",
   "name": "conda-env-python2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
